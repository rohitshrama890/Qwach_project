{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AehswtgQ0Fgk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijVNaWlOSFnt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dGhmMHtPZAm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3280,
     "status": "ok",
     "timestamp": 1738897738726,
     "user": {
      "displayName": "MANISH KUMAR",
      "userId": "14782295773613732595"
     },
     "user_tz": -330
    },
    "id": "t-VZXWHxYaDz",
    "outputId": "a6745b0b-c515-432b-b143-e004442e4a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting whois\n",
      "  Downloading whois-1.20240129.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading whois-1.20240129.2-py3-none-any.whl (61 kB)\n",
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/61.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: whois\n",
      "Successfully installed whois-1.20240129.2\n"
     ]
    }
   ],
   "source": [
    "!pip install whois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1738897847954,
     "user": {
      "displayName": "MANISH KUMAR",
      "userId": "14782295773613732595"
     },
     "user_tz": -330
    },
    "id": "mJWrHS7xRLEo",
    "outputId": "bbc62ef2-34f5-4a49-de95-a18735189ec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracted URL: 168    https://www.dailymail.co.uk\n",
      "Name: url, dtype: object\n",
      "\n",
      "===== Extracted URL Features =====\n",
      "Have_IP: False\n",
      "Have_At: False\n",
      "URL_Length: 59\n",
      "URL_Depth: 2\n",
      "Redirection: True\n",
      "https_Domain: True\n",
      "TinyURL: False\n",
      "Prefix/Suffix: False\n",
      "DNS_Record: 1\n",
      "Web_Traffic: 1\n",
      "Domain_Age: 0\n",
      "Domain_End: 0\n",
      "iFrame: 0\n",
      "Mouse_Over: 0\n",
      "Right_Click: 0\n",
      "Web_Forwards: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pyzbar.pyzbar import decode\n",
    "\n",
    "def scan_qr_code(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    qr_data = decode(image)\n",
    "\n",
    "    if not qr_data:\n",
    "        return None  # No QR code detected\n",
    "\n",
    "    return qr_data[0].data.decode(\"utf-8\")  # Extracted URL\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/content/benign_168.png\"  # Change to your QR image path\n",
    "scanned_url = scan_qr_code(image_path)\n",
    "print(\"üîç Extracted URL:\", scanned_url)\n",
    "import re\n",
    "import socket\n",
    "import requests\n",
    "import whois\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain_info = tldextract.extract(url)\n",
    "\n",
    "    try:\n",
    "        domain_whois = whois.whois(parsed_url.netloc)  # WHOIS Lookup\n",
    "    except:\n",
    "        domain_whois = None\n",
    "\n",
    "    # 1Ô∏è‚É£ Basic URL Features\n",
    "    features = {\n",
    "        \"Have_IP\": bool(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed_url.netloc)),\n",
    "        \"Have_At\": \"@\" in url,\n",
    "        \"URL_Length\": len(url),\n",
    "        \"URL_Depth\": url.count('/'),\n",
    "        \"Redirection\": \"//\" in url[7:],\n",
    "        \"https_Domain\": \"https\" in domain_info.domain,\n",
    "        \"TinyURL\": any(short in url.lower() for short in [\"bit.ly\", \"tinyurl\", \"goo.gl\"]),\n",
    "        \"Prefix/Suffix\": \"-\" in parsed_url.netloc\n",
    "    }\n",
    "\n",
    "    # 2Ô∏è‚É£ Domain-Based Features\n",
    "    try:\n",
    "        socket.gethostbyname(parsed_url.netloc)  # Check if DNS record exists\n",
    "        features[\"DNS_Record\"] = 1\n",
    "    except:\n",
    "        features[\"DNS_Record\"] = 0\n",
    "\n",
    "    try:\n",
    "        alexa_rank = requests.get(f\"https://www.alexa.com/siteinfo/{parsed_url.netloc}\").status_code  # Check if site exists\n",
    "        features[\"Web_Traffic\"] = 1 if alexa_rank == 200 else 0\n",
    "    except:\n",
    "        features[\"Web_Traffic\"] = 0\n",
    "\n",
    "    # Domain Age & Expiry\n",
    "    if domain_whois:\n",
    "        try:\n",
    "            domain_age = (domain_whois.creation_date[0] if isinstance(domain_whois.creation_date, list) else domain_whois.creation_date)\n",
    "            domain_expiry = (domain_whois.expiration_date[0] if isinstance(domain_whois.expiration_date, list) else domain_whois.expiration_date)\n",
    "            features[\"Domain_Age\"] = (domain_expiry - domain_age).days if domain_age and domain_expiry else 0\n",
    "            features[\"Domain_End\"] = (domain_expiry - domain_age).days if domain_expiry else 0\n",
    "        except:\n",
    "            features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "    else:\n",
    "        features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "\n",
    "    # 3Ô∏è‚É£ Web Content Features (if accessible)\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # iFrame Detection\n",
    "        features[\"iFrame\"] = 1 if \"<iframe\" in response.text else 0\n",
    "\n",
    "        # Mouse Over Detection\n",
    "        features[\"Mouse_Over\"] = 1 if \"onmouseover\" in response.text else 0\n",
    "\n",
    "        # Right Click Disabled\n",
    "        features[\"Right_Click\"] = 1 if \"event.button==2\" in response.text else 0\n",
    "\n",
    "        # Web Forwarding Detection\n",
    "        features[\"Web_Forwards\"] = len(response.history) > 2  # If multiple redirects\n",
    "\n",
    "    except:\n",
    "        features[\"iFrame\"], features[\"Mouse_Over\"], features[\"Right_Click\"], features[\"Web_Forwards\"] = 0, 0, 0, 0\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features from scanned URL\n",
    "if scanned_url:\n",
    "    url_features = extract_url_features(scanned_url)\n",
    "\n",
    "    print(\"\\n===== Extracted URL Features =====\")\n",
    "    for key, value in url_features.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1355362,
     "status": "ok",
     "timestamp": 1738901186900,
     "user": {
      "displayName": "MANISH KUMAR",
      "userId": "14782295773613732595"
     },
     "user_tz": -330
    },
    "id": "2CgZ8-N1P2O4",
    "outputId": "53e07393-4902-4fee-8511-ade4a7484c6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious QR codes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2011/2011 [11:23<00:00,  2.94it/s]\n",
      "Processing benign QR codes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2001/2001 [11:10<00:00,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Features extracted and saved to 'qr_features.csv'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import socket\n",
    "import requests\n",
    "import whois\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from pyzbar.pyzbar import decode\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# üìå Define dataset root path\n",
    "DATASET_PATH = \"/content/drive/MyDrive/QR codes\"  # Change to your dataset path\n",
    "\n",
    "# üìå Function to scan QR code and extract URL\n",
    "def scan_qr_code(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    qr_data = decode(image)\n",
    "\n",
    "    if not qr_data:\n",
    "        return None  # No QR code detected\n",
    "\n",
    "    return qr_data[0].data.decode(\"utf-8\")  # Extracted URL\n",
    "\n",
    "# üìå Function to extract URL features\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain_info = tldextract.extract(url)\n",
    "\n",
    "    try:\n",
    "        domain_whois = whois.whois(parsed_url.netloc)  # WHOIS Lookup\n",
    "    except:\n",
    "        domain_whois = None\n",
    "\n",
    "    # 1Ô∏è‚É£ Basic URL Features\n",
    "    features = {\n",
    "        \"Have_IP\": bool(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed_url.netloc)),\n",
    "        \"Have_At\": \"@\" in url,\n",
    "        \"URL_Length\": len(url),\n",
    "        \"URL_Depth\": url.count('/'),\n",
    "        \"Redirection\": \"//\" in url[7:],\n",
    "        \"https_Domain\": \"https\" in domain_info.domain,\n",
    "        \"TinyURL\": any(short in url.lower() for short in [\"bit.ly\", \"tinyurl\", \"goo.gl\"]),\n",
    "        \"Prefix/Suffix\": \"-\" in parsed_url.netloc\n",
    "    }\n",
    "\n",
    "    # 2Ô∏è‚É£ Domain-Based Features\n",
    "    try:\n",
    "        socket.gethostbyname(parsed_url.netloc)  # Check if DNS record exists\n",
    "        features[\"DNS_Record\"] = 1\n",
    "    except:\n",
    "        features[\"DNS_Record\"] = 0\n",
    "\n",
    "    try:\n",
    "        alexa_rank = requests.get(f\"https://www.alexa.com/siteinfo/{parsed_url.netloc}\").status_code  # Check if site exists\n",
    "        features[\"Web_Traffic\"] = 1 if alexa_rank == 200 else 0\n",
    "    except:\n",
    "        features[\"Web_Traffic\"] = 0\n",
    "\n",
    "    # Domain Age & Expiry\n",
    "    if domain_whois:\n",
    "        try:\n",
    "            domain_age = (domain_whois.creation_date[0] if isinstance(domain_whois.creation_date, list) else domain_whois.creation_date)\n",
    "            domain_expiry = (domain_whois.expiration_date[0] if isinstance(domain_whois.expiration_date, list) else domain_whois.expiration_date)\n",
    "            features[\"Domain_Age\"] = (domain_expiry - domain_age).days if domain_age and domain_expiry else 0\n",
    "            features[\"Domain_End\"] = (domain_expiry - domain_age).days if domain_expiry else 0\n",
    "        except:\n",
    "            features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "    else:\n",
    "        features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "\n",
    "    # 3Ô∏è‚É£ Web Content Features (if accessible)\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # iFrame Detection\n",
    "        features[\"iFrame\"] = 1 if \"<iframe\" in response.text else 0\n",
    "\n",
    "        # Mouse Over Detection\n",
    "        features[\"Mouse_Over\"] = 1 if \"onmouseover\" in response.text else 0\n",
    "\n",
    "        # Right Click Disabled\n",
    "        features[\"Right_Click\"] = 1 if \"event.button==2\" in response.text else 0\n",
    "\n",
    "        # Web Forwarding Detection\n",
    "        features[\"Web_Forwards\"] = len(response.history) > 2  # If multiple redirects\n",
    "\n",
    "    except:\n",
    "        features[\"iFrame\"], features[\"Mouse_Over\"], features[\"Right_Click\"], features[\"Web_Forwards\"] = 0, 0, 0, 0\n",
    "\n",
    "    return features\n",
    "\n",
    "# üìå Process all QR code images in dataset\n",
    "dataset_features = []\n",
    "\n",
    "# Loop through each folder inside \"dataset/\"\n",
    "for category_folder in os.listdir(DATASET_PATH):\n",
    "    folder_path = os.path.join(DATASET_PATH, category_folder)\n",
    "\n",
    "    # Ignore non-directory files\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    # üìå Assign label dynamically from folder name\n",
    "    label = 1 if \"malicious\" in category_folder.lower() else 0  # 1 = Malicious, 0 = Safe\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path), desc=f\"Processing {category_folder} QR codes\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Scan QR code\n",
    "        scanned_url = scan_qr_code(image_path)\n",
    "        if scanned_url:\n",
    "            features = extract_url_features(scanned_url)\n",
    "            features[\"Label\"] = label  # Assign label dynamically\n",
    "            dataset_features.append(features)\n",
    "\n",
    "# üìå Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(dataset_features)\n",
    "df.to_csv(\"qr_features.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Features extracted and saved to 'qr_features.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkoBFTIMgblU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'scispacy (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n scispacy ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pyzbar.pyzbar import decode\n",
    "\n",
    "def scan_qr_code(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    qr_data = decode(image)\n",
    "\n",
    "    if not qr_data:\n",
    "        return None  # No QR code detected\n",
    "\n",
    "    return qr_data[0].data.decode(\"utf-8\")  # Extracted URL\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/content/benign_168.png\"  # Change to your QR image path\n",
    "scanned_url = scan_qr_code(image_path)\n",
    "print(\"üîç Extracted URL:\", scanned_url)\n",
    "import re\n",
    "import socket\n",
    "import requests\n",
    "import whois\n",
    "import tldextract\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain_info = tldextract.extract(url)\n",
    "\n",
    "    try:\n",
    "        domain_whois = whois.whois(parsed_url.netloc)  # WHOIS Lookup\n",
    "    except:\n",
    "        domain_whois = None\n",
    "\n",
    "    # 1Ô∏è‚É£ Basic URL Features\n",
    "    features = {\n",
    "        \"Have_IP\": bool(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed_url.netloc)),\n",
    "        \"Have_At\": \"@\" in url,\n",
    "        \"URL_Length\": len(url),\n",
    "        \"URL_Depth\": url.count('/'),\n",
    "        \"Redirection\": \"//\" in url[7:],\n",
    "        \"https_Domain\": \"https\" in domain_info.domain,\n",
    "        \"TinyURL\": any(short in url.lower() for short in [\"bit.ly\", \"tinyurl\", \"goo.gl\"]),\n",
    "        \"Prefix/Suffix\": \"-\" in parsed_url.netloc\n",
    "    }\n",
    "\n",
    "    # 2Ô∏è‚É£ Domain-Based Features\n",
    "    try:\n",
    "        socket.gethostbyname(parsed_url.netloc)  # Check if DNS record exists\n",
    "        features[\"DNS_Record\"] = 1\n",
    "    except:\n",
    "        features[\"DNS_Record\"] = 0\n",
    "\n",
    "    try:\n",
    "        alexa_rank = requests.get(f\"https://www.alexa.com/siteinfo/{parsed_url.netloc}\").status_code  # Check if site exists\n",
    "        features[\"Web_Traffic\"] = 1 if alexa_rank == 200 else 0\n",
    "    except:\n",
    "        features[\"Web_Traffic\"] = 0\n",
    "\n",
    "    # Domain Age & Expiry\n",
    "    if domain_whois:\n",
    "        try:\n",
    "            domain_age = (domain_whois.creation_date[0] if isinstance(domain_whois.creation_date, list) else domain_whois.creation_date)\n",
    "            domain_expiry = (domain_whois.expiration_date[0] if isinstance(domain_whois.expiration_date, list) else domain_whois.expiration_date)\n",
    "            features[\"Domain_Age\"] = (domain_expiry - domain_age).days if domain_age and domain_expiry else 0\n",
    "            features[\"Domain_End\"] = (domain_expiry - domain_age).days if domain_expiry else 0\n",
    "        except:\n",
    "            features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "    else:\n",
    "        features[\"Domain_Age\"], features[\"Domain_End\"] = 0, 0\n",
    "\n",
    "    # 3Ô∏è‚É£ Web Content Features (if accessible)\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # iFrame Detection\n",
    "        features[\"iFrame\"] = 1 if \"<iframe\" in response.text else 0\n",
    "\n",
    "        # Mouse Over Detection\n",
    "        features[\"Mouse_Over\"] = 1 if \"onmouseover\" in response.text else 0\n",
    "\n",
    "        # Right Click Disabled\n",
    "        features[\"Right_Click\"] = 1 if \"event.button==2\" in response.text else 0\n",
    "\n",
    "        # Web Forwarding Detection\n",
    "        features[\"Web_Forwards\"] = len(response.history) > 2  # If multiple redirects\n",
    "\n",
    "    except:\n",
    "        features[\"iFrame\"], features[\"Mouse_Over\"], features[\"Right_Click\"], features[\"Web_Forwards\"] = 0, 0, 0, 0\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features from scanned URL\n",
    "if scanned_url:\n",
    "    url_features = extract_url_features(scanned_url)\n",
    "\n",
    "    print(\"\\n===== Extracted URL Features =====\")\n",
    "    for key, value in url_features.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOUcgezTSMXX3JylSczX+6o",
   "mount_file_id": "1TBoTbWZz8v4ItJCeQd1kd83b6CEKGDLU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "scispacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
